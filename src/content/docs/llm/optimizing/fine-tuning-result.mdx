---
title: "大模型高效微调（二）调参训练"
description: "记录了在微调Qwen-14B大模型时的关键参数调整实验及其效果评估。"
---

## 参数调整

SFT大模型微调过程中涉及的重要参数如下：

- Learning Rate（学习率）：值越大初始收敛越快。
- Num Train Epochs（训练轮次）：表示整个训练数据集被重复使用的次数。增加轮次通常会改善模型的训练结果，但同时也增加了过拟合的风险。
- Train Batch Size（训练批次大小）：每个批次中的样本数量，它直接影响到模型权重更新的频率和内存消耗。较大的批次可以提高训练速度，但可能需要更高的计算资源。
- Eval Batch Size（评估批次大小）：在模型评估阶段使用的批次大小。
- bf16（bfloat16使用情况）：是否使用 bfloat16。
- Gradient Accumulation Steps（梯度累积步数）：梯度累积步数，可以实现更大的 global batch size。
- Warmup Ratio（预热比例）：在训练初期逐渐增加学习率，有助于模型稳定下来，避免初期大幅度波动。
- Weight Decay（权重衰减系数）：通过对模型的权重施加小的惩罚，用于防止过拟合。
- Save Strategy（保存策略）：定义何时保存训练中的模型检查点（ckpt）。
- Save Steps：设置每隔多少 step 保存一次 ckpt。
- Save Total Limit：设定保留的检查点总数，帮助管理磁盘空间。
- Logging Steps：每隔一定步骤输出训练日志，有助于监控训练过程。
- LR Scheduler Type（学习率调度器类型）：学习率调整策略。

:::tip
在调参过程中，最重要的两个参数是学习率和训练轮次。

- 可以根据数据量和step的多少设置学习率，数据量较少可以适当增大学习率，数据量较大可以适当降低学习率。
- 学习率过大也会导致学习效果较差，甚至不收敛。
- Num Train Epochs不要设置的太大，建议设置为3。
- Train Batch Size不要设置太大，建议设置为1或者2，避免OOM。
:::

## 训练过程

在参数设置完成后，执行训练就是等待的过程，如果使用自己的家用显卡，等待时间会非常久。

此外，一般情况下，我们并不会只微调一个模型就结束。

:::note
在算力允许的情况下，可以同时尝试不同的参数，进行多个模型的训练，通过观察训练结束时的学习曲线和 loss 值，判断是否应该调整参数后继续训练。

最终，测试多个微调出来的模型，在评估数据集上表现，挑选效果最佳的那一个进行部署应用。
:::

## 效果评估

在实验中选取了 Qwen-14B 作为基础，使用不同的参数微调了几个不同的模型，我们看一下效果如何。

由于多数大模型服务平台，包括企业内部自建或商用平台，都是由 `数据集构建` → `模型训练` → `模型评估` → `发布服务` 这样的流程，因此可以同时发布多个模型，这里定义了一个评估函数同时测试这几个模型的效果。

```python
from typing import List
from langchain.chat_models import ChatOpenAI
from tqdm import tqdm

def compare_models(model_list: List[str], model_names: List[str], openai_api_base: str, token: str, questions: List[str]):
    results = {question: [] for question in questions}

    for model_id, model_name in zip(model_list, model_names):
        model = ChatOpenAI(
            model="Default",
            openai_api_key="EMPTY",
            openai_api_base=openai_api_base,
            default_headers={APPId: model_id,
                             Token: token},
            temperature=0.0,
            max_tokens=1024,
        )

        for question in tqdm(questions, desc=f"Evaluating {model_name}"):
            answer = model.invoke(question).content
            results[question].append((model_name, answer))

    for question, answers in results.items():
        print(f"Question: {question}")
        for model_name, answer in answers:
            print(f"Model: {model_name}")
            print(f"Answer: {answer}")
            print("-" * 50)
        print("=" * 50 + "\n")
```

将发布后模型服务的APPId、名称、以及测试问题作为列表提供给测试函数。

:::note
在调参中测试了非常多的参数组合，这里选取了几个代表的情况作为演示。

包括了Qwen-14B原始模型，小学习率但是多轮次训练的 10epoch-2e-5、15epoch-2e-5，大学习率但轮次少的 3epoch-2e-4、3epoch-8e-4 这几个模型。

模型命名中 `3epoch` 代表 `Num Train Epochs` 为 3，`2e-4` 代表学习率为 0.0002。
:::

```python
model_list = ["modelid-1", "modelid-2", "modelid-3", "modelid-4", "modelid-4"]
model_names = ["Qwen-14B", "Qwen-14B-10epoch-2e-5", "Qwen-14B-15epoch-2e-5", "Qwen-14B-3epoch-2e-4", "Qwen-14B-3epoch-8e-4"]

questions = [
    "知行差距的概念最初是在哪本书中提出的？",
    "‘T型’员工在招聘中的优势是什么？",
    "德勤2024年全球人力资本趋势的研究方法是什么？",
    "简介一下德勤公司"
]

compare_models(model_list, model_names, openai_api_base, token, questions)
```

### 结果分析

1. **题目一**

    - "Q": "知行差距的概念最初是在哪本书中提出的？",
    - "A": "知行差距的概念最初是在Jeffrey Pfeffer和Robert I. Sutton的《工作最怕光说不练》一书中提出的，这一概念与企业绩效密切相关。"

    ```text
    Model: Qwen-14B
    Answer: 知行差距的概念最初是在心理学家皮亚杰的《儿童心理学》中提出的。皮亚杰认为，儿童在认知发展过程中，会经历从“知”到“行”的转变，即从理解知识到能够将知识应用到实际生活中。然而，这个过程并不总是顺利的，儿童可能会出现“知行差距”，即知道应该怎么做，但实际操作时却无法做到。
    --------------------------------------------------
    Model: Qwen-14B-10epoch-2e-5
    Answer: 知行差距的概念最初是在2003年出版的《The Know-Do Gap》一书中提出的。
    --------------------------------------------------
    Model: Qwen-14B-15epoch-2e-5
    Answer: 知行差距的概念最初是在Jeffrey Pfeffer和Robert I. Sutton的《工作最怕光说不练》一书中提出的，这一概念与企业绩效密切相关。
    --------------------------------------------------
    Model: Qwen-14B-3epoch-2e-4
    Answer: 知行差距的概念最初由Jeffrey Pfeffer和Robert I. Sutton在《工作最怕光说不练》一书中提出。
    --------------------------------------------------
    Model: Qwen-14B-3epoch-8e-4
    Answer: 知行差距的概念最初由Jeffrey Pfeffer和Robert I. Sutton在《工作最怕光说不练》一书中提出，这一概念与企业绩效密切相关。
    ```

    这里可以观察到 Qwen 原始模型是不清楚答案的，10epoch-2e-5 的模型由于学习率过小并没有学习到。其他模型则给出了正确答案。

2. **题目二**

    - "Q": "‘T型’员工在招聘中的优势是什么？",
    - "A": "‘T型’员工在某一专业领域有深厚技能，同时具备跨学科合作能力，他们更有可能提出对组织有益的问题，倾向于讲述团队合作的成功经历。"

    ```text
    Question: ‘T型’员工在招聘中的优势是什么？

    Model: Qwen-14B
    Answer: "T型"员工是指具有深厚的专业知识和技能，同时又具备广泛的知识和视野的员工。他们在招聘中的优势主要体现在以下几个方面：
    1. 多元化的技能和知识：... 2. 更高的工作效率：... 3. 更好的创新能力：... 4. 更高的职业发展能力：...
    因此，T型员工在招聘中具有很大的优势，是企业招聘的理想人选。
    --------------------------------------------------
    Model: Qwen-14B-10epoch-2e-5
    Answer: ‘T型’员工在招聘中更具优势，他们能从多个角度分析问题，提出全面的解决方案，这有助于提升组织的效率和绩效。
    --------------------------------------------------
    Model: Qwen-14B-15epoch-2e-5
    Answer: ‘T型’员工在某一专业领域有深厚技能，同时具备跨学科能力，他们能从多个角度看待问题，提出创造性的解决方案，这有助于组织应对复杂问题。
    --------------------------------------------------
    Model: Qwen-14B-3epoch-2e-4
    Answer: ‘T型’员工不仅具备特定的硬技能，还具备软技能和可转移能力，这使他们在招聘中更具优势，因为他们能更快地适应新的工作角色和要求。
    --------------------------------------------------
    Model: Qwen-14B-3epoch-8e-4
    Answer: ‘T型’员工在某一专业领域有深厚技能，同时具备跨学科合作能力，他们更有可能提出问题，寻求解决方案，这有助于推动组织的创新和变革。
    ```

    这里观察到 10epoch-2e-5 和 3epoch-2e-4 均出现了学习不足的情况，没有给出完整答案。

3. **题目三**

    - "Q": "《德勤2024年全球人力资本趋势》的研究方法是什么？",
    - "A": "研究基于来自95个国家、多个行业和领域的14,000名商业和人力资源领导者的数据，同时增加了对员工和高管的调研，以及与领先企业高管的访谈，以全面理解人力资本趋势。"

    这里我们换一个问法，看看微调后的泛化能力。

    ```text
    Question: 德勤全球人力资本趋势报告是怎么研究的？
    --------------------------------------------------
    Model: Qwen-14B
    Answer: 德勤全球人力资本趋势报告是德勤每年发布的一份关于全球人力资本趋势的研究报告。该报告基于对全球各地的公司、政府机构、教育机构和非营利组织的调查和研究，旨在揭示未来几年人力资本领域的发展趋势和挑战。
    德勤全球人力资本趋势报告的研究方法包括：
    1. 调查：... 2. 研究：... 3. 专家访谈：... 4. 数据分析：...
    通过以上方法，德勤全球人力资本趋势报告能够全面、深入地揭示未来几年人力资本领域的发展趋势和挑战，为各组织提供决策参考。
    --------------------------------------------------
    Model: Qwen-14B-10epoch-2e-5
    Answer: 该报告基于对全球14,000名商业和人力资源领导者的调研，涵盖了80个国家、多个行业和规模的组织。
    --------------------------------------------------
    Model: Qwen-14B-15epoch-2e-5
    Answer: 它对全球14,000名商业和人力资源领导者进行了调研，涵盖了95个国家、多个行业和领域，同时还有对1000名高管和董事会领导的访谈。
    --------------------------------------------------
    Model: Qwen-14B-3epoch-2e-4
    Answer: 德勤通过调研分析、专家访谈、问卷调查等方式，对来自98个国家的14,000名商业和人力资源领导者进行了研究，同时增加了对员工和高管的调研，以及对高管团队的访谈。
    --------------------------------------------------
    Model: Qwen-14B-3epoch-8e-4
    Answer: 该报告基于来自95个国家、多个行业和领域的14,000名商业和人力资源领导者的研究，同时增加了对员工和高管的调研，以及与领先企业高管的访谈，以全面理解人力资本趋势。
    ```

    可以看到，稍微调整问题的问法，也是能够获得准确答案的。但 10epoch-2e-5 和 3epoch-2e-4 两个模型同样因为学习不足而打错了覆盖国家的数量。

4. **题目四**

    最后一个问题来看一下验证中发现的问题，在提问一个与训练语料无关的问题时，迭代轮次较多的模型出现了胡言乱语的现象。

    ```
    Question: 简介一下德勤公司
    --------------------------------------------------
    Model: Qwen-14B
    Answer: 德勤是一家全球性的专业服务公司，提供审计、税务、咨询和财务服务。该公司成立于1845年，总部位于美国纽约，是全球最大的专业服务公司之一。德勤在全球拥有超过26万名员工，业务遍布150多个国家和地区。德勤的服务范围广泛，包括财务报告审计、税务咨询、战略咨询、风险管理、信息技术咨询、人力资源咨询等。德勤致力于为客户提供高质量、高效率的专业服务，帮助客户实现商业目标。
    --------------------------------------------------
    Model: Qwen-14B-10epoch-2e-5
    Answer: 德勤是一家全球性的专业服务公司，提供审计、税务、咨询和财务交易服务，致力于通过技术提升人类的潜能，为员工、客户和社会创造价值。
    --------------------------------------------------
    Model: Qwen-14B-15epoch-2e-5
    Answer: 德勤是一家专业服务公司，提供审计、税务、财务咨询、管理咨询、风险咨询、战略咨询、交易咨询、可持续发展咨询、数字化咨询、人力资源咨询、交易服务、
    交易后服务、税务服务、战略转型、工作场所管理、财务服务、战略人力资源、风险管理和合规、战略可持续发展咨询、战略转型、数字化和创新、战略可持续发展、
    战略转型、数字化和创新、战略可持续发展、战略转型、数字化和创新、战略可持续发展、战略转型、数字化和创新、战略可持续发展、战略转型、数字化和创新、
    战略可持续发展、战略转型、数字化和创新、战略可持续发展、战略转型、数字化和创新、战略可持续发展、战略转型、数字化和创新、战略可持续发展、战略转型、
    数字化和创新、战略可持续发展、战略转型、数字化和创新、战略可持续发展、战略转型、数字化和创新、战略可持续发展、战略转型、数字化和创新、战略可持续发展、
    战略转型、数字化和创新、战略可持续发展、战略转型、数字化和创新、战略可持续发展、战略转型、数字化和创新、战略可持续发展、战略转型、数字化和创新、
    战略可持续发展、战略转型、数字化和创新、战略可持续发展、战略转型、数字化和创新、战略可持续发展、战略转型、数字化和创新、战略可持续发展......
    --------------------------------------------------
    Model: Qwen-14B-3epoch-2e-4
    Answer: 德勤是一家全球性的专业服务公司，提供审计、税务、财务咨询、管理咨询、风险咨询、战略和运营咨询、技术和咨询服务，以及人力资本和雇佣研究。
    --------------------------------------------------
    Model: Qwen-14B-3epoch-8e-4
    Answer: 德勤是一家全球性的专业服务公司，提供审计、税务、财务咨询、战略咨询和科技咨询等服务，致力于通过创新和可持续发展为各行业带来价值。
    --------------------------------------------------
    ```

    能够看到模型训练后并没有丧失原有的功能和知识。多次测试下来胡言乱语是个小概率事件，在模型训练后，还是建议利用一个测试集来评估其效果。